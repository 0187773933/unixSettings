#!/usr/bin/env python
"""
export-saved.py

Christopher Su
Exports saved Reddit posts into a HTML file that is ready to be imported into Google Chrome.
"""

from time import time
import argparse
import csv
import logging
import sys

import praw

import json
import distutils.dir_util
from os import path
from collections import defaultdict
homeFolder = path.expanduser("~")
backupFolder = path.join( homeFolder , "BACKUP" , "Reddit" )
savePath = path.join( backupFolder , "036.json" )
distutils.dir_util.mkpath( backupFolder )

saveMaster = { "comments": None , "saved": None , "upvoted": None , "submissions": None  }
saveOBJ = defaultdict( lambda : None )

__version__ = '0.1.2'


REDDIT_USERNAME = ''
REDDIT_PASSWORD = ''
CLIENT_ID = ''
CLIENT_SECRET = ''

# # Converter class from https://gist.github.com/raphaa/1327761
class Converter():
	"""Converts a CSV instapaper export to a Chrome bookmark file."""

	def __init__(self, file, html_file=None, folder_name="Reddit"):
		"""init method."""
		self._file = file
		self._html_file = html_file if html_file is not None else 'chrome-bookmarks.html'
		self._folder_name = folder_name if folder_name is not None else 'Reddit'

	def parse_urls(self):
		"""Parse the file and returns a folder ordered list."""
		efile = open(self._file)
		urls = csv.reader(efile, dialect='excel')
		parsed_urls = {}
		next(urls)
		for url in urls:
			if not url:
				continue
			else:
				folder = url[4].strip()
			if folder not in list(parsed_urls.keys()):
				parsed_urls[folder] = []
			parsed_urls[folder].append([url[0], url[1], url[2]])
		return parsed_urls

	def convert(self):
		"""Convert the file."""
		urls = self.parse_urls()
		t = int(time())
		content = ('<!DOCTYPE NETSCAPE-Bookmark-file-1>\n'
				   '<META HTTP-EQUIV="Content-Type" CONTENT="text/html;'
				   ' charset=UTF-8">\n<TITLE>Bookmarks</TITLE>'
				   '\n<H1>Bookmarks</H1>\n<DL><P>\n<DT><H3 ADD_DATE="%(t)d"'
				   ' LAST_MODIFIED="%(t)d">%(folder_name)s</H3>'
				   '\n<DL><P>\n' % {'t': t, 'folder_name': self._folder_name})

		for folder in sorted(list(urls.keys())):
			content += ('<DT><H3 ADD_DATE="%(t)d" LAST_MODIFIED="%(t)d">%(n)s'
						'</H3>\n<DL><P>\n' % {'t': t, 'n': folder})
			for url, title, add_date in urls[folder]:
				content += ('<DT><A HREF="%(url)s" ADD_DATE="%(created)d"'
							' LAST_MODIFIED="%(created)d">%(title)s</A>\n'
							% {'url': url, 'created': int(add_date), 'title': title})
			content += '</DL><P>\n'
		content += '</DL><P>\n' * 3
		ifile = open(self._html_file, 'wb')
		try:
			ifile.write(content)
		except TypeError:
			ifile.write(content.encode('utf-8', 'ignore'))



def login():
	"""login method.

	Args:
		args (argparse.Namespace): Parsed arguments.

	Returns: a logged on praw instance
	"""
	# login
	reddit = praw.Reddit(client_id=CLIENT_ID,
						 client_secret=CLIENT_SECRET,
						 user_agent='export saved 2.0',
						 username=REDDIT_USERNAME,
						 password=REDDIT_PASSWORD)
	logging.info('Login succesful')
	return reddit


def get_csv_rows(reddit, seq):
	global saveOBJ
	"""get csv rows.

	Args:
		reddit: reddit praw's instance
		seq (list): List of Reddit item.

	Returns:
		list: Parsed reddit item.
	"""
	csv_rows = []
	reddit_url = reddit.config.reddit_url

	# filter items for link
	for idx, i in enumerate(seq, 1):
		logging.info('processing item #{}'.format(idx))

		if not hasattr(i, 'title'):
			i.title = i.link_title

		# Fix possible buggy utf-8
		#title = i.title.encode('utf-8').decode('utf-8')
		#title = i.title.encode('utf-8')
		# try:
		# 	logging.info('title: {}'.format(title))
		# except UnicodeEncodeError:
		# 	logging.info('title: {}'.format(title.encode('utf8', 'ignore')))

		try:
			created = int(i.created)
		except ValueError:
			created = 0

		try:
			folder = str(i.subreddit).encode('utf-8').decode('utf-8')
		except AttributeError:
			folder = "None"

		if callable(i.permalink):
			permalink = i.permalink()
		else:
			permalink = i.permalink
		#permalink = permalink.encode('utf-8').decode('utf-8')

		#csv_rows.append([reddit_url + permalink, title, created, None, folder])
		title = i.title.encode( 'utf-8' )
		if saveOBJ[ folder ] is None:
			saveOBJ[ folder ] = []
		saveOBJ[ folder ].append( { "url": reddit_url + permalink , "title": title , "created": created } )

	return csv_rows


def write_csv(csv_rows, file_name=None):
	"""write csv using csv module.

	Args:
		csv_rows (list): CSV rows.
		file_name (string): filename written
	"""
	file_name = file_name if file_name is not None else 'export-saved.csv'

	# csv setting
	csv_fields = ['URL', 'Title', 'Created', 'Selection', 'Folder']
	delimiter = ','

	# write csv using csv module
	try:
		with open(file_name, "wb") as f:
			csvwriter = csv.writer(f, delimiter=delimiter, quoting=csv.QUOTE_MINIMAL)
			csvwriter.writerow(csv_fields)
			for row in csv_rows:
				csvwriter.writerow(row)
	except (UnicodeEncodeError, TypeError) as e:
		with open(file_name, "w") as f:
			csvwriter = csv.writer(f, delimiter=delimiter, quoting=csv.QUOTE_MINIMAL)
			csvwriter.writerow(csv_fields)
			for row in csv_rows:
				try:
					csvwriter.writerow(row)
				except UnicodeEncodeError:
					csvwriter.writerow([r.encode('utf-8', 'ignore')
										if isinstance(r, str) else r for r in row])


def process(reddit, seq, file_name, folder_name):
	global saveOBJ
	global saveMaster
	"""Write csv and html from a list of results.
	Args:
		reddit: reddit praw's instance
		seq (list): list to write
		file_name: base file name without extension
		folder_name: top level folder name for the exported html bookmarks
	"""
	csv_rows = get_csv_rows(reddit, seq)
	# write csv using csv module

	#write_csv(csv_rows, file_name + ".csv")
	print( "Saving to --> " + savePath )
	saveMaster[ file_name ] = saveOBJ
	with open( savePath , "w+" ) as outfile:
		json.dump( saveMaster , outfile )
	saveOBJ = defaultdict( lambda : None )

	# logging.info('csv written.')
	# # convert csv to bookmark
	# converter = Converter(file_name + ".csv", file_name + ".html",
	# 					  folder_name=folder_name)
	# converter.convert()
	# logging.info('html written.')


def save_upvoted(reddit):
	""" save upvoted posts """
	seq = reddit.user.me().upvoted(limit=None)
	process(reddit, seq, "upvoted", "Reddit - Upvoted")


def save_saved(reddit):
	""" save saved posts """
	seq = reddit.user.me().saved(limit=None)
	process(reddit, seq, "saved", "Reddit - Saved")


def save_comments(reddit):
	""" save comments posts """
	seq = reddit.user.me().comments.new(limit=None)
	process(reddit, seq, "comments", "Reddit - Comments")


def save_submissions(reddit):
	""" save saved posts """
	seq = reddit.user.me().submissions.new(limit=None)
	process(reddit, seq, "submissions", "Reddit - Submissions")


def main():
	#"""main func."""
	# args = get_args(sys.argv[1:])
	# print( args )
	# # set logging config
	# if args.verbose:
	# 	logging.basicConfig(level=logging.DEBUG)

	# # print program version.
	# if args.version:
	# 	print(__version__)
	# 	return

	reddit = login()
	save_upvoted(reddit)
	save_saved(reddit)
	save_submissions(reddit)
	save_comments(reddit)

	sys.exit(0)


if __name__ == "__main__":  # pragma: no cover
	main()
